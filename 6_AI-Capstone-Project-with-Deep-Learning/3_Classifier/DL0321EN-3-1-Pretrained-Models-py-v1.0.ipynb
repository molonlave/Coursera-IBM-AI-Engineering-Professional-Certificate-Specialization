{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a href=\"https://cognitiveclass.ai\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/Logos/organization_logo/organization_logo.png\" width = 400> </a>\n",
    "\n",
    "<h1 align=center><font size = 5>Pre-Trained Models</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3> \n",
    "    \n",
    "1. <a href=\"#item31\">Import Libraries and Packages</a>\n",
    "2. <a href=\"#item32\">Download Data</a>  \n",
    "3. <a href=\"#item33\">Define Global Constants</a>  \n",
    "4. <a href=\"#item34\">Construct ImageDataGenerator Instances</a>  \n",
    "5. <a href=\"#item35\">Compile and Fit Model</a>\n",
    "\n",
    "</font>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item31'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Import Libraries and Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Let's start the lab by importing the libraries that we will be using in this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item32'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "For your convenience, I have placed the data on a server which you can retrieve easily using the **wget** command. So let's run the following line of code to get the data. Given the large size of the image dataset, it might take some time depending on your internet speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-03-02 07:05:32--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip\n",
      "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
      "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 261482368 (249M) [application/zip]\n",
      "Saving to: ‘concrete_data_week3.zip’\n",
      "\n",
      "concrete_data_week3 100%[===================>] 249.37M  26.6MB/s    in 11s     \n",
      "\n",
      "2020-03-02 07:05:43 (23.0 MB/s) - ‘concrete_data_week3.zip’ saved [261482368/261482368]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## get the data\n",
    "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "And now if you check the left directory pane, you should see the zipped file *concrete_data_week3.zip* appear. So, let's go ahead and unzip the file to access the images. Given the large number of images in the dataset, this might take a couple of minutes, so please be patient, and wait until the code finishes running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!unzip concrete_data_week3.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50*** error. So please **DO NOT DO IT**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item33'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Define Global Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Here, we will define constants that we will be using throughout the rest of the lab. \n",
    "\n",
    "1. We are obviously dealing with two classes, so *num_classes* is 2. \n",
    "2. The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n",
    "3. We will training and validating the model using batches of 100 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "image_resize = 224\n",
    "\n",
    "batch_size_training = 100\n",
    "batch_size_validation = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item34'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Construct ImageDataGenerator Instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next, we will use the *flow_from_directory* method to get the training images as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/train',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "## Type your answer here\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Double-click __here__ for the solution.\n",
    "<!-- The correct answer is:\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')\n",
    "-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item35'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Build, Compile and Fit Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In this section, we will start building our model. We will use the Sequential model class from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94658560/94653016 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model.add(ResNet50(\n",
    "    include_top=False,\n",
    "    pooling='avg',\n",
    "    weights='imagenet',\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 2048)              23587712  \n",
      "=================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 23,534,592\n",
      "Non-trainable params: 53,120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "You can access the model's layers using the *layers* attribute of our model object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.training.Model at 0x7f681a87ae48>,\n",
       " <keras.layers.core.Dense at 0x7f67ffa6b4e0>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "You can access the ResNet50 layers by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x7f68d4cfb908>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f68d4cfbba8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f68d4daefd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f68d4dae780>,\n",
       " <keras.layers.core.Activation at 0x7f68d4d04208>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f68f13400f0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f68cfaafef0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f68cfb62240>,\n",
       " <keras.layers.core.Activation at 0x7f68cfc10fd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f68cfc2e2b0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f689c664b00>,\n",
       " <keras.layers.core.Activation at 0x7f685c7c6320>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f685c75fa20>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f685c7343c8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f685c74eb38>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f685c641f28>,\n",
       " <keras.layers.merge.Add at 0x7f685c38fb70>,\n",
       " <keras.layers.core.Activation at 0x7f685c305ef0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f685c305d30>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f685c26a4a8>,\n",
       " <keras.layers.core.Activation at 0x7f685c240d68>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f685c231f60>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f685c225dd8>,\n",
       " <keras.layers.core.Activation at 0x7f685c196f28>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f685c0c37b8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f685c1328d0>,\n",
       " <keras.layers.merge.Add at 0x7f685c09e160>,\n",
       " <keras.layers.core.Activation at 0x7f685c033c88>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f685c033e48>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f684079aef0>,\n",
       " <keras.layers.core.Activation at 0x7f6840705128>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f68406d08d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f68406c39e8>,\n",
       " <keras.layers.core.Activation at 0x7f68406ae278>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f68405d6f28>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f68cfa64358>,\n",
       " <keras.layers.merge.Add at 0x7f68cfa3cf98>,\n",
       " <keras.layers.core.Activation at 0x7f68cf90f6a0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f68cf90f400>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f68d4a750f0>,\n",
       " <keras.layers.core.Activation at 0x7f68d4d692b0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f68d4a306d8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f68d4c6b550>,\n",
       " <keras.layers.core.Activation at 0x7f68d4d0a5c0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f68d4c625f8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f68402f0c50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f68d4a65ba8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f684023cf60>,\n",
       " <keras.layers.merge.Add at 0x7f68401c1b38>,\n",
       " <keras.layers.core.Activation at 0x7f68401b2f98>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f68401b2dd8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6840131ef0>,\n",
       " <keras.layers.core.Activation at 0x7f68400986a0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6840056f60>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6828f7aa58>,\n",
       " <keras.layers.core.Activation at 0x7f6828f53630>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6828f32a90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6828f23ba8>,\n",
       " <keras.layers.merge.Add at 0x7f6828e50e10>,\n",
       " <keras.layers.core.Activation at 0x7f6828e34438>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6828e34160>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6828d879e8>,\n",
       " <keras.layers.core.Activation at 0x7f6828d67b00>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6828cc9dd8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6828cbaef0>,\n",
       " <keras.layers.core.Activation at 0x7f6828c0b1d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6828bda668>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6828bc6780>,\n",
       " <keras.layers.merge.Add at 0x7f6828b79a58>,\n",
       " <keras.layers.core.Activation at 0x7f6828ac9fd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f6828ac9e10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6828ab9fd0>,\n",
       " <keras.layers.core.Activation at 0x7f6828a206d8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f68289860f0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f689ef0eef0>,\n",
       " <keras.layers.core.Activation at 0x7f682890fcc0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f68288e8f28>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6828842940>,\n",
       " <keras.layers.merge.Add at 0x7f6828813550>,\n",
       " <keras.layers.core.Activation at 0x7f68287daac8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f68287da908>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f68287800f0>,\n",
       " <keras.layers.core.Activation at 0x7f6828708f98>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f682867b4a8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f682863ee10>,\n",
       " <keras.layers.core.Activation at 0x7f682861c860>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f682857ecf8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f68284c4128>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f68285f1e10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f682847e6a0>,\n",
       " <keras.layers.merge.Add at 0x7f682846b7f0>,\n",
       " <keras.layers.core.Activation at 0x7f682837e828>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f682837e668>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f6828357eb8>,\n",
       " <keras.layers.core.Activation at 0x7f68282e5048>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f68282ace80>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f682829df98>,\n",
       " <keras.layers.core.Activation at 0x7f68281ee2b0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f682813a710>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f68281a9828>,\n",
       " <keras.layers.merge.Add at 0x7f68280dbb00>,\n",
       " <keras.layers.core.Activation at 0x7f68280accf8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f68280acef0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f681bfdfeb8>,\n",
       " <keras.layers.core.Activation at 0x7f681bfbbb70>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f681bf21e48>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f681bf13f60>,\n",
       " <keras.layers.core.Activation at 0x7f681be5f278>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f681be2e6d8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f681be1c7f0>,\n",
       " <keras.layers.merge.Add at 0x7f681bd4ea90>,\n",
       " <keras.layers.core.Activation at 0x7f681bd1fcc0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f681bd1feb8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f681bc8be80>,\n",
       " <keras.layers.core.Activation at 0x7f681bc66b38>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f681bbcce10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f681bc3ef28>,\n",
       " <keras.layers.core.Activation at 0x7f681bb0e240>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f681bade6a0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f681bacc7b8>,\n",
       " <keras.layers.merge.Add at 0x7f681ba7da58>,\n",
       " <keras.layers.core.Activation at 0x7f681b9cbc88>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f681b9cbe48>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f681b9bb780>,\n",
       " <keras.layers.core.Activation at 0x7f681b917b00>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f681b8f8dd8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f681b8e9ef0>,\n",
       " <keras.layers.core.Activation at 0x7f681b83d208>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f681b78b668>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f681b7f6780>,\n",
       " <keras.layers.merge.Add at 0x7f681b726a58>,\n",
       " <keras.layers.core.Activation at 0x7f681b6f8fd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f681b6f8e10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f681b667ef0>,\n",
       " <keras.layers.core.Activation at 0x7f681b5c5ac8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f681b5a6da0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f681b598eb8>,\n",
       " <keras.layers.core.Activation at 0x7f681b4eb1d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f681b4b7630>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f681b4a4748>,\n",
       " <keras.layers.merge.Add at 0x7f681b3d59e8>,\n",
       " <keras.layers.core.Activation at 0x7f681b3a8f98>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f681b3a8dd8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f681b315eb8>,\n",
       " <keras.layers.core.Activation at 0x7f681b2f0a90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f681b251d68>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f681b244e80>,\n",
       " <keras.layers.core.Activation at 0x7f681b196198>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f681b1635f8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f681b0829b0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f681b14e710>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f681b057f60>,\n",
       " <keras.layers.merge.Add at 0x7f681afc4f60>,\n",
       " <keras.layers.core.Activation at 0x7f681af50d68>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f681af50f28>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f681aecf6d8>,\n",
       " <keras.layers.core.Activation at 0x7f681aeabbe0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f681ae15780>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f681ae03898>,\n",
       " <keras.layers.core.Activation at 0x7f681adb4b70>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f681ad19fd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f681acf4748>,\n",
       " <keras.layers.merge.Add at 0x7f681ac5e400>,\n",
       " <keras.layers.core.Activation at 0x7f681ac1a978>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f681ac1a7b8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f681ab4ffd0>,\n",
       " <keras.layers.core.Activation at 0x7f681ab79470>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f681aac0748>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f681ab30860>,\n",
       " <keras.layers.core.Activation at 0x7f681aa64b38>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f681a9c7f98>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f681a99e748>,\n",
       " <keras.layers.merge.Add at 0x7f681a90b390>,\n",
       " <keras.layers.core.Activation at 0x7f681a8c9940>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x7f681a8c9780>,\n",
       " <keras.layers.pooling.GlobalAveragePooling2D at 0x7f681a898ef0>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 2048)              23587712  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 4098      \n",
      "=================================================================\n",
      "Total params: 23,591,810\n",
      "Trainable params: 4,098\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next we compile our model using the **adam** optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "steps_per_epoch_training = len(train_generator)\n",
    "steps_per_epoch_validation = len(validation_generator)\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " 34/301 [==>...........................] - ETA: 2:09:18 - loss: 0.2908 - acc: 0.8747"
     ]
    }
   ],
   "source": [
    "fit_history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_training,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=steps_per_epoch_validation,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now that the model is trained, you are ready to start using it to classify images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.save('classifier_resnet_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Thank you for completing this lab!\n",
    "\n",
    "This notebook was created by Alex Aklson. I hope you found this lab interesting and educational."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3_LAB1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr>\n",
    "\n",
    "Copyright &copy; 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
